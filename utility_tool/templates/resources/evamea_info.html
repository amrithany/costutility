{% extends 'index.html' %}
{% load static %}
{% block body_block %}                                                                                                                                                                                           
<div class="hero-unit">
<br>
<br>
<b>Evaluation Measures and Results: </b><font color="blue">Resources to help identify existing evidence or collect new evidence to evaluate how well each Solution Option meets your Evaluation Criteria</font>
<br><br>
<b><font color="blue">Identifying Evaluation Measures and Data to Collect</b></font>
<br><br> To assess how well each Solution Option you are considering meets your stated Evaluation Criteria, you will need common measures to evaluate the options against each criterion. For example, if the criterion relates to improving performance on a standardized test, the evaluation measure might be “score on standardized test” or “gain in score on standardized test since last Academic Year.”  
<br><br><i>DecisionMaker</i> suggests ways to evaluate Solution Options against many Evaluation Criteria but, if you need additional ideas, the resources listed below may be helpful in finding existing data or collecting your own data.
<br><br>Some examples of existing information you can gather to evaluate Solution Options include:
<li>        Efficacy studies or process evaluations published in peer-reviewed journals or other publications</li>  
<li>      Information from the program vendor</li>  
<li>     Data collected on outcomes in places where the Solution Options have been implemented previously, either in your own context or in other contexts</li>  
<br> You may also choose to collect your own information by:
<li>     Piloting Solution Options and surveying teachers or students for feedback</li>  
<li>     Collecting data on outcomes</li>  
<li>     Holding focus groups with stakeholders to understand buy-in</li>
<br> If you don’t have much time before a decision must be made, you may need to rely on data from other contexts such as other districts or universities and use this to make an informed estimate of what results you could expect in your own context. Your confidence in being able to replicate the results from another location in your own location might be affected by factors such as the similarity of conditions and populations 
served, or whether the Solution Option was used in multiple locations and performed similarly across all of them. The Relevance and Credibility section below provides more guidance about this.
<br><br>If no relevant data are available or can be collected within your timeframe for making the decision, you could elicit professional judgments from experienced staff members about how well each Solution Option will perform against each criterion. For example, you could ask Counselors to review college planning software intended for high schoolers and ask them to estimate the percentage increase in high schoolers in your district who will apply to college if you implemented each of several alternative software options in your high schools.
<br><br><b><font color="blue">Relevance and Credibility</b></font>
<br><br> In some cases, decision-makers may want to use available evidence to evaluate Solution Options against Evaluation Criteria. However, decision-makers may struggle to determine whether the available evidence is relevant or even appropriate to use in their context.
<br><br> Additionally, in the particular case of assessing evidence of effectiveness, prior studies may exist, but these studies are likely to vary in quality or credibility. Decision-makers may want to consider the results of a less rigorous study to assess whether a Solution Option is effective, but may want to account for the fact that the study could be over- or under- estimating the size of the impact.
<br><br>To account for these issues, <i>DecisionMaker</i> provides two optional indices to evaluate existing evidence: a Relevance Index and a Credibility Index. To learn more about these indices and how to use them, please click <b><a href="{% static 'CBCSE Relevance and Credibility Indices_042419.pdf' %}" target="_blank">here</a></b>.
<br><br><b><font color="blue">Assigning values to Solution Options for each evaluation criterion</b></font>
<br><br>Once you have identified the measures you will use to assess each Solution Option against each Evaluation Criterion and have the data you need to do so in hand, you will need to assign values on each measure for each Solution Option and enter these values into <i>DecisionMaker</i>. 
<br><br><b><font color="blue">Data must be in numerical form</b></font>
<br><br>Data must be collected in a numerical form in order for <i>DecisionMaker</i> to calculate a utility value, e.g., points on a 0-10 scale, percentage points, hours per week, days per year, or rubric scores. If you have qualitative data, for example, rubric ratings of “Meets grammar standards to the highest expected level,” “Meets most grammar standards at an acceptable level,” etc., you will need to assign a numerical score to each rubric rating.
<br><br><b><font color="blue">Likely lowest and likely highest scores</b></font>
<br><br><i>DecisionMaker</i> uses your expectations about likely best and worst case scenarios with respect to each evaluation measure to establish the high and low bounds of utility. The best case scenario you provide is set at 10 out of 10 for utility, assuming that you will be totally satisfied if a Solution Option performs at this level. Similarly, the worst case scenario is set at 0 out of 10 for utility, assuming you will be very dissatisfied with a Solution Option that performs at the lowest likely level. For example, if the evaluation measure is a test of reading comprehension scored between 0 and 40, you would enter the lowest score you think a student in the decision context might feasibly earn, and the highest score.  If the population you are trying to serve is struggling readers, you might enter a likely lowest score of 10 and a likely highest score of 25.  If they are your advanced readers, the range might be 30-40. These ranges might be based on your past experiences reviewing data from this test, or based on benchmarks provided by the test’s developer/vendor.
<br><br><b><font color="blue">Setting the direction of your preferences</b></font>
<br><br>For some evaluation measures such as test scores, graduation rate, or time on task, you are likely to prefer higher values. For others, such as days of absence, dropout rate, or suspensions, you are likely to prefer lower values. <i>DecisionMaker</i> asks you to indicate whether higher values are better for each Evaluation Criterion in order to establish whether utility increases for you and your stakeholders as the values go up or down. 
<br><br><b><font color="blue">Resources to help you evaluate Solution Options </b></font>
<br><br>The free, online resources listed below can be used to help find existing evidence on educational programs, strategies and tools, or to help you produce new evidence to evaluate your Solution Options. <b><a href="https://ed.gov/policy/elsec/leg/essa/guidanceuseseinvestment.pdf" target="_blank">Using Evidence to Strengthen Education Investments</a></b> is a useful introduction to thinking about evaluating educational programs and strategies.
<br><br>
<h4 style="border: 2px solid grey;padding-left: 15px;background: LightBlue;">  
    Resources to help you conduct your own evaluations of Solution Options 
<br><br></h4>
<b><a href="https://www.qualtrics.com/blog/10-tips-for-building-effective-surveys/" target="_blank">Best Practices in Survey Design from Qualtrics:</a></b> Some tips on how to design questions for your own surveys.
<br><br>
<b><a href="http://www.cebc4cw.org/" target="_blank">California Evidence-based Clearinghouse:</a></b> Provides descriptions and information on research evidence on child welfare programs, and guidance on how to choose and implement programs.
<br><br>
<b><a href="https://edtech.digitalpromise.org/" target="_blank">Digital Promise’s Ed Tech Pilot Framework:</a></b> Provides an 8-step framework for district leaders to plan and conduct pilots on educational technology products, along with research-based tools and resources, and tips collected from 14 districts.
<br><br>
<b><a href="https://edtechrce.org/" target="_blank">Mathematica’s Ed Tech RCE Coach:</a></b> Facilitates the design and analysis of evaluations of educational technology and the interpretation of the results using data on the group using the technology and a comparison group. Although originally designed for evaluating educational technology, RCE Coach can be used for any educational program.
<br><br>
<b><a href="https://rct-yes.com/" target="_blank">RCT-Yes:</a></b> Facilitates the estimation and reporting of program effects using randomized controlled trials or other evaluation designs with comparison groups. Note that users need to download and install the software as well as R or Stata.
<br><br>
<b><a href="https://ies.ed.gov/ncee/edlabs/projects/technicalAssistance.asp" target="_blank">Regional Education Laboratories:</a></b>  Regional Education Laboratories (RELs) can serve as a “thought partner” for evaluations of educational programs or initiatives. They also offer training, coaching, and technical support (TCTS) for research use “in the form of in-person or virtual consultation or training on research design, data collection or analysis, or approaches for selecting or adapting research-based interventions to new contexts.”
<br><br>
<h4 style="border: 2px solid grey;padding-left: 15px;background: LightBlue;">
Resources to help you find existing evidence on educational programs, strategies and tools            
<br><br></h4>  

<b><a href="http://www.bestevidence.org/" target="_blank">Best Evidence Encyclopedia (BEE):</a></b> Provides summaries of scientific reviews produced by many authors and organizations and links to full texts of each review.
<br><br>
<b><a href="https://www.campbellcollaboration.org/" target="_blank">Campbell Collaboration:</a></b> Provides systematic reviews of research evidence.
<br><br>
<b><a href="https://www.cochranelibrary.com/" target="_blank">Cochrane Library:</a></b> Provides systematic reviews of intervention effectiveness and diagnosis tests related to public health in evaluation settings.
<br><br>
<b><a href="https://researchmap.digitalpromise.org/" target="_blank">Digital Promise Research Map:</a></b> Provides links to reviews and articles of educational research on 12 teaching and learning topics, and organizes them in Network View and Chord View to show connections among them.
<br><br>
<b><a href="https://ies.ed.gov/ncee/edlabs/" target="_blank">Regional Education Laboratories:</a></b> School districts or State Education Agencies can contact their REL for help with gathering evidence on educational strategies and programs. Upon request, RELs will perform reference searches and informal reviews of the existing research on interventions, and/or of specific studies against What Works Clearinghouse standards.  
<br><br>
<b><a href="https://ies.ed.gov/ncee/wwc/" target="_blank">What Works Clearinghouse:</a></b> Provides reviews of existing research on different programs, products, practices, and policies in education, and summaries of findings.
<br><br>
 <b><a href="https://scholar.google.com/" target="_blank">Google Scholar:</a></b> You can search by key word for scholarly publications on any topic including peer-reviewed articles, evaluation reports, and white papers.  These have not been vetted by Google, so you will need to carefully assess the sources and credibility of any documents you find here. Note that peer-reviewed publications often prefer that authors avoid naming specific programs and products.  Also, some peer-reviewed journals may be behind a paywall.  Sometimes you can find a free, pre-print version of the same paper on the internet by copying and pasting the title of the paper into your search engine.
 <br><br>
{% if whereamI == 'aboutmaker' %}                                                                                                                                                                                
<a href="/utility_tool/resources/about_maker.html"><input class="btn btn-success" type="button" name="cancel" value="Back to About DecisionMaker"></a>
{% else %}  
    {% if dec_id != 0 %}
        <a href="/utility_tool/decisions/{{ dec_id }}/menu.html"><input class="btn btn-success" type="button" name="cancel" value="Back to Decision Flowchart"></a>
    {% endif %}   
    <a href="/utility_tool/decisions/solution_options/guidance.html"><input class="btn btn-success" type="button" name="cancel2" value="Back to Resources & Guidance"></a> 
    {% endif %}    
</div>
{% endblock %}
